# main.py
import argparse
from ocr import ocr_multiple_pdfs
from chunking import chunk_all_pdfs
from embeddings import build_faiss_from_ocr
from graph import build_graph
from entities import enrich_graph_with_entities
from llm_query_and_guardrail import answer_query
from config import OCR_CHUNKS_FOLDER, SUPPORTED_EXTENSIONS
from pathlib import Path

def main(args):
    # -----------------------------
    # INGESTION PIPELINE
    # -----------------------------
    if args.ingest:
        print("\nüöÄ Starting document ingestion pipeline...\n")

        # 0Ô∏è‚É£ Collect PDF files
        pdf_paths = [str(p) for p in Path(args.pdf_folder).glob("*") if p.suffix.lower() in SUPPORTED_EXTENSIONS]
        if not pdf_paths:
            print("‚ùå No PDF files found in folder:", args.pdf_folder)
            return

        print("0Ô∏è‚É£ Performing OCR on PDFs")
        ocr_multiple_pdfs(pdf_paths)

        print("\n1Ô∏è‚É£ Chunking PDFs")
        chunks = chunk_all_pdfs()
        print(f"‚úÖ Total chunks prepared: {len(chunks)}")

        print("\n2Ô∏è‚É£ Building embeddings (FAISS)")
        index, metadata = build_faiss_from_ocr()
        print(f"‚úÖ FAISS index created with {len(metadata)} chunks")

        print("\n3Ô∏è‚É£ Ingesting chunks into Neo4j")
        build_graph(chunks)
        print("‚úÖ Neo4j graph populated")

        print("\n4Ô∏è‚É£ Extracting entities & enriching graph")
        enrich_graph_with_entities()
        print("‚úÖ Entities extracted and graph enriched")

        print("\nüéâ Ingestion pipeline completed successfully.\n")

    # -----------------------------
    # QUERY LOOP
    # -----------------------------
    print("üí¨ Hybrid Knowledge Agent is ready to answer questions.")
    while True:
        query = input("\nAsk a question (or type 'exit'): ")
        if query.lower() == "exit":
            break

        result = answer_query(query)

        print("\nüìå FINAL ANSWER")
        print(result["answer"])

        print("\nüîç PROVENANCE")
        for c in result["chunks_used"]:
            print(f"- {c['doc_id']} | Page {c['page_number']} | Chunk {c['chunk_id']}")

        print("\nüõ°Ô∏è GUARDRAIL")
        print(result["guardrail"])


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Hybrid LLM Knowledge Agent")
    parser.add_argument(
        "--ingest",
        action="store_true",
        help="Run full ingestion pipeline (OCR ‚Üí chunking ‚Üí embeddings ‚Üí graph ‚Üí entities)"
    )
    parser.add_argument(
        "--pdf_folder",
        type=str,
        default="pdfs",
        help="Folder containing PDFs to ingest"
    )

    args = parser.parse_args()
    main(args)
